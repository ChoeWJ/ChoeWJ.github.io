---
layout: single
title: "[ML-DL] 위클리 페이퍼 - 편향과 분산"
categories:
  - ML-DL
  - ML
tag: [ML-DL]
author_profile: false
sidebar:
  nav: "counts"
# protect: true
contact_info: choewj117@gmail.com
# search: false
# redirect_from:
#   - /coding/first-posting
use_math: true
---

# 모델 학습 시 발생할 수 있는 편향과 분산에 대해 설명하고, 두 개념의 관계에 대해 설명해 주세요.

편향-분산 트레이드오프는 머신러닝에서 모델의 예측력을 높이고 새로운 데이터에 잘 적용되도록 만드는 <font color="#1E90FF">핵심 원리</font>다. 모델이 너무 단순하면 데이터의 본질을 놓치고, 너무 복잡하면 학습 데이터에만 지나치게 맞춰진다. 이 미묘한 균형을 맞추는 과정은 머신러닝의 성공을 좌우한다.

## 편향과 분산: 모델 복잡도의 양면

편향-분산 트레이드오프는 <font color="#FFD700">모델 복잡도</font>가 예측 정확성과 일반화 능력에 어떻게 영향을 미치는지를 보여준다. 모델이 복잡해질수록 학습 데이터에 더 잘 맞춰지지만, 새로운 데이터에 대한 예측은 불안정해질 수 있다.

- **편향(Bias)**: 편향은 알고리즘이 잘못된 가정을 했을 때 발생하는 오류다. 예를 들어, 비선형 데이터를 선형 모델로 예측하려 하면 <font color="#32CD32">과소적합</font>이 생기며, 데이터의 핵심 패턴을 놓친다. 높은 편향은 단순한 모델에서 흔히 나타난다.
- **분산(Variance)**: 분산은 학습 데이터의 작은 변화에 모델이 과도하게 반응할 때 생기는 오류다. 복잡한 모델은 데이터의 노이즈까지 학습해 <font color="#FF4500">과적합</font>을 초래할 수 있다. 높은 분산은 모델이 학습 데이터에 지나치게 의존할 때 나타난다.
- **복잡도와의 관계**: 매개변수가 많아지면 모델은 더 유연해져 학습 데이터에 잘 맞는다. 하지만 이는 <font color="#1E90FF">분산 증가</font>를 동반하며, 새로운 데이터에 대한 예측이 불안정해질 수 있다. 반대로 단순한 모델은 분산은 낮지만 편향이 높아진다.

편향과 분산은 서로 상충하는 관계로, 모델 복잡도를 조절해 이 둘의 균형을 찾는 것이 핵심이다.

## 편향-분산 분해: 오류의 구조를 풀다

편향-분산 분해는 <font color="#FFD700">평균 제곱 오차(MSE)</font>를 세 가지 요소로 나누어 모델의 오류를 분석하는 수학적 접근이다. 이는 모델이 새로운 데이터에서 얼마나 잘 작동할지를 이해하는 데 필수적이다.

### 분해의 구성 요소

- **편향**: 모델 예측값의 평균과 실제 함수의 기대값 간 차이로, 모델이 데이터의 본질적 패턴을 얼마나 잘 포착했는지를 보여준다. 높은 편향은 모델이 너무 단순함을 나타낸다.
- **분산**: 예측값의 변동성을 측정하며, 데이터셋 변화에 따라 예측이 얼마나 달라지는지를 나타낸다. 높은 분산은 모델이 노이즈에 과민하게 반응했음을 뜻한다.
- **감소 불가능한 오류**: 데이터 자체의 노이즈로, 어떤 모델로도 줄일 수 없는 오류다. 이는 <font color="#32CD32">잡음의 분산(σ²)</font>으로 표현된다.

### 수학적 전개

평균 제곱 오차(MSE)는 다음과 같이 정의된다:

$ \text{MSE} = \mathbb{E} [(y - \hat{f}(x;D))^2] $

여기서 $\( y = f(x) + \varepsilon \)$는 실제값, $\( \hat{f}(x;D) \)$는 모델의 예측값, $\( \varepsilon \)$는 평균이 0이고 분산이 $\( \sigma^2 \)$인 잡음이다. 이를 편향-분산 분해로 나누면:

$ \text{MSE} = \text{Bias}^2 + \text{Variance} + \sigma^2 $

- **Bias²**: $ (\mathbb{E}[\hat{f}(x;D)] - f(x))^2 $로, 예측값의 평균과 실제 함수 간 차이를 제곱한 값이다.
- **Variance**: $ \mathbb{E} [(\hat{f}(x;D) - \mathbb{E}[\hat{f}(x;D)])^2] $로, 예측값의 변동성을 나타낸다.
- **σ²**: 감소 불가능한 잡음의 분산이다.

이 분해는 모델이 복잡해질수록 <font color="#FF4500">편향은 줄지만 분산은 증가</font>한다는 점을 명확히 드러낸다.

### 시각적 이해

편향-분산 분해를 시각적으로 풀어보면:

<img src="{{ '/assets/images/bias-variance.png' | relative_url }}" alt="이미지 설명">

- <font color="#green">초록색</font>: 실제 테스트 라벨 값으로, 이들의 분산은 감소 불가능한 오류(σ²)를 나타낸다.
- <font color="royalblue">파란색</font>: 무작위 훈련 데이터셋에서 예측한 테스트 라벨 값으로, 이들의 분산은 모델의 분산을 보여준다.
- <font color="red">Total error</font>: 편향으로, 예측값의 평균과 실제값 간 차이를 나타낸다.

이 시각적 표현은 MSE가 편향, 분산, 감소 불가능한 오류의 합임을 직관적으로 이해하도록 돕는다.

## 모델 복잡도 조절: 트레이드오프 최적화

편향-분산 트레이드오프를 최적화하려면 <font color="#1E90FF">모델 복잡도</font>를 적절히 조절해야 한다. 이를 위한 주요 방법과 사례를 살펴보자.

### 정규화와 단순화

- **정규화**: <font color="#32CD32">LASSO</font>나 <font color="#32CD32">릿지 회귀</font> 같은 기법은 모델에 편향을 의도적으로 도입해 분산을 줄인다. 이는 일반 최소 제곱법(OLS)보다 낮은 MSE를 제공할 수 있다.
- **차원 축소와 특징 선택**: 불필요한 특징을 제거해 모델을 단순화하면 분산이 감소한다.
- **더 큰 훈련 데이터**: 데이터량이 많아지면 모델의 분산이 줄어들고, 편향이 낮은 모델을 학습해도 오류가 최소화된다.

### k-최근접 이웃 회귀 사례

k-최근접 이웃(KNN) 회귀 모델은 편향-분산 트레이드오프를 잘 보여준다:

- **작은 k**: 모델이 데이터에 민감하게 반응해 <font color="#FF4500">분산은 크고 편향은 작다</font>.
- **큰 k**: 모델이 단순해져 <font color="#FFD700">편향은 증가하지만 분산은 줄어든다</font>.
- 합리적인 가정하에, $k=1(1-NN)$에서는 훈련 데이터가 무한히 많아질수록 편향이 사라진다.

### 신경망과 하이퍼파라미터

인공신경망에서는 숨겨진 유닛 수를 늘리면 <font color="#1E90FF">편향은 줄지만 분산이 증가</font>한다. 반대로, 드롭아웃 같은 정규화를 적용하면 분산을 줄이고 약간의 편향을 도입한다. 하이퍼파라미터 최적화는 이 균형을 맞추는 데 필수적이다.

## 다양한 분야에서의 편향-분산 트레이드오프

편향-분산 트레이드오프는 머신러닝을 넘어 여러 분야에서 중요한 통찰을 제공한다.

- **강화학습**: 제한된 환경 정보 속에서 알고리즘의 비최적성은 비대칭적 편향과 과적합으로 나눌 수 있다. <font color="#32CD32">마르코프 체인 몬테카를로(MCMC)</font>는 비대칭적 편향을 줄이는 데 기여한다.
- **인지 과학**: 인간은 높은 편향과 낮은 분산의 휴리스틱을 사용해 제한된 데이터로 일반화를 이룬다. 이는 <font color="#FFD700">Homo Heuristicus</font> 연구에서 탐구된다.
- **객체 인식**: 모델-프리 접근은 고차원 데이터에서 높은 분산을 유발하므로, 초기 단계에서 편향을 도입하는 하드 와이어링이 필요하다.
