---
layout: single
title: "[ML-DL] 위클리 페이퍼 - 손실 함수"
categories:
  - ML-DL
  - ML
tag: [ML-DL]
author_profile: false
sidebar:
  nav: "counts"
# protect: true
contact_info: choewj117@gmail.com
# search: false
# redirect_from:
#   - /coding/first-posting
use_math: true
---

# 손실 함수(loss function)란 무엇이며, 왜 중요한가요?

## 손실 함수란?

손실 함수는 <font color="#1E90FF">모델의 예측값과 실제값 간의 차이</font>를 수학적으로 계산하는 도구로, 모델이 얼마나 정확한지를 한눈에 보여준다. 이 차이를 정량화함으로써 모델이 잘못된 예측을 줄이고 데이터에 더 잘 맞도록 학습할 수 있는 길을 제시한다. 손실 함수의 역할은 단순히 점수를 매기는 데 그치지 않는다. 학습 과정에서 매개변수를 조정해 <font color="#FF4500">오차를 최소화</font>하도록 방향을 잡아주며, 모델이 과소적합이나 과적합에 빠지지 않도록 균형을 유지하는 데 기여한다.

또한, 손실 함수는 데이터의 이상치에 얼마나 민감하게 반응할지, 어떤 오류를 더 중요하게 다룰지를 결정한다. 예를 들어, 이상치가 큰 영향을 미치는 상황에서는 특정 손실 함수가 모델의 견고성을 높여줄 수 있다. 전체 데이터셋에 대한 손실 함수의 평균인 <font color="#32CD32">비용 함수</font>는 모델의 전반적인 성능을 평가하며, 학습이 올바른 궤적을 따르는지 확인하는 데 필수적이다.

## 회귀와 분류에서의 손실 함수

손실 함수는 머신러닝 문제의 성격에 따라 다르게 설계된다. <font color="#1E90FF">회귀 문제</font>에서는 연속적인 값을 예측하는 데 초점을 맞추고, <font color="#FF4500">분류 문제</font>에서는 이산적인 클래스 레이블을 정확히 맞추는 것이 목표다. 이 두 가지 문제를 해결하기 위해 다양한 손실 함수가 사용되며, 각각의 특성과 적용 사례를 알아보자.

### 회귀 문제에 적합한 손실 함수

회귀 문제에서는 예측값과 실제값의 차이를 최소화하는 손실 함수가 필요하다. 대표적인 손실 함수들을 살펴보면 다음과 같다.

- **평균 제곱 오차(MSE)**: 예측값과 실제값의 <font color="#FFD700">제곱 오차를 평균</font>한 값으로, 큰 오차에 더 많은 패널티를 부여한다. 이상치에 민감하기 때문에, 부동산 가격 예측처럼 높은 값의 정확도가 중요한 경우에 적합하다. MSE는 모델이 큰 편차를 줄이도록 강하게 유도하며, 회귀 작업의 표준 손실 함수로 자리 잡았다.
- **평균 절대 오차(MAE)**: <font color="#32CD32">절대 오차를 평균</font>한 값으로, 이상치에 덜 민감하다. 모든 오차에 균등한 가중치를 부여하므로, 음식 배달 시간 예측처럼 이상치를 과도하게 벌하지 않아야 하는 상황에서 유리하다.
- **Huber 손실**: MSE와 MAE의 장점을 결합한 손실 함수로, 작은 오차에는 MSE처럼 민감하게, 큰 오차에는 MAE처럼 덜 민감하게 반응한다. <font color="#FF4500">델타(δ)</font> 파라미터를 통해 민감도를 조정할 수 있어, 이상치와 작은 오차를 균형 있게 다루고자 할 때 적합하다.

### 분류 문제에 적합한 손실 함수

분류 문제에서는 클래스 레이블의 예측 확률을 기반으로 손실을 계산한다. 주요 손실 함수는 다음과 같다.

- **이진 교차 엔트로피(Binary Cross-Entropy)**: 이진 분류에서 실제값과 예측값 간의 <font color="#1E90FF">확률 분포 차이</font>를 로그로 계산한다. 흔히 로그 손실(Log Loss)이라고도 불리며, 로지스틱 회귀나 신경망에서 널리 사용된다. 이 손실 함수는 모델이 두 클래스 간의 분포를 정확히 맞추도록 돕는다.
- **힌지 손실(Hinge Loss)**: <font color="#FFD700">최대 여백 분류</font>를 목표로 설계된 손실 함수로, 데이터 포인트와 결정 경계 간의 여백을 최대화한다. 잘못된 분류에 패널티를 부여해 모델의 일반화 능력을 높이며, 서포트 벡터 머신(SVM)에서 주로 활용된다.

## 손실 함수 선택의 중요성과 기준

손실 함수는 단순한 계산 도구가 아니라, 모델이 데이터를 어떻게 이해하고 학습할지를 결정하는 <font color="#32CD32">내부 나침반</font>이다. 적절한 손실 함수를 선택하면 학습 알고리즘이 손실을 최소화하며 데이터의 근본적인 패턴을 포착하고, 새로운 데이터에도 잘 일반화될 수 있다. 반대로 잘못된 선택은 모델을 엉뚱한 방향으로 이끌거나, 과적합과 과소적합의 함정에 빠뜨릴 수 있다.

### 선택 시 고려해야 할 요소

손실 함수를 선택할 때는 몇 가지 핵심 요소를 고려해야 한다.

- **문제 유형에 따른 선택**: 분류 문제에서는 <font color="#FF4500">교차 엔트로피</font>가 적합하며, 이진 분류에는 이진 교차 엔트로피, 다중 클래스 분류에는 범주형 교차 엔트로피를 사용한다. 회귀 문제에서는 MSE, MAE, 또는 Huber 손실이 주로 선택된다.
- **이상치 민감도**: 이상치가 모델에 미치는 영향을 고려해야 한다. MSE는 이상치에 민감해 큰 오차를 강조하지만, MAE는 이상치를 드문 이벤트로 간주하며 균등한 오차 처리를 선호한다. Huber 손실은 이 둘의 균형을 맞춘다.
- **계산 효율성**: 대규모 데이터셋에서는 <font color="#1E90FF">계산 비용</font>이 중요한 요소다. MSE와 MAE는 구현이 간단하고 효율적이지만, 복잡한 손실 함수는 계산 자원을 많이 소모할 수 있다.
- **모델의 일반화 능력**: 손실 함수는 모델이 새로운 데이터에 얼마나 잘 작동할지를 결정한다. 힌지 손실처럼 여백을 최대화하는 함수는 일반화 성능을 높이는 데 유리하다.

### 이상치와의 싸움

이상치는 데이터셋의 전반적인 분포를 벗어나는 값으로, 모델의 성능에 큰 영향을 미칠 수 있다. MSE는 이상치에 높은 패널티를 부여해 모델이 이를 줄이는 방향으로 학습하지만, 이는 때로는 과도한 조정으로 이어질 수 있다. 반면, MAE는 이상치를 덜 강조하며 안정적인 학습을 돕는다. Huber 손실은 이 두 접근을 절충해, 이상치의 영향을 적절히 관리하며 모델의 견고성을 높인다.

## 손실 함수의 구현과 실전 활용

손실 함수는 이론뿐 아니라 실제 구현에서도 중요한 역할을 한다. 현대 머신러닝 라이브러리는 손실 함수를 쉽게 사용할 수 있도록 지원하며, 개발 과정을 간소화한다.

- **주요 손실 함수**: MSE, MAE, Huber 손실, 이진 교차 엔트로피, 힌지 손실, 로그 손실 등이 널리 사용된다.
- **구현 예시**: Scikit-learn에서는 `mean_squared_error`와 `mean_absolute_error`로 MSE와 MAE를 간단히 계산할 수 있다. TensorFlow와 PyTorch는 GPU 가속과 병렬 컴퓨팅을 지원해 대규모 데이터셋에서도 효율적으로 손실 함수를 처리한다.
- **라이브러리의 장점**: 이러한 라이브러리는 사용 편의성과 최적화된 성능을 제공하며, 개발자가 프로젝트 요구 사항에 따라 맞춤형 손실 함수를 설계할 수도 있다.
